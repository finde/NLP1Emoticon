\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor,colortbl}
\usepackage[]{algorithm2e}

\usepackage[backend=bibtex]{biblatex}
\addbibresource{ourbib.bib}

%\usepackage{soul}

\newcommand{\tab}{\hspace{10mm}}
\newcommand{\dtab}{\hspace{20mm}}
\newcommand{\ttab}{\hspace{30mm}}
\newcommand{\qtab}{\hspace{40mm}}

\begin{document}

\title{Natural Language Processing \\ Emoticon Prediction}

\author{By: Gornishka, Loor, Qodari, Xumara}
\maketitle


\tableofcontents

\pagebreak



% IVAN WANTS BOTH ABSTRACT AND INTRODUCTION. The old introduction sounds more like abstract maybe. But then what should we put in the introduction??? :/

%%%%%%%%%%%%%%%%
%%%%%  Abstract  %%%%%%
%%%%%%%%%%%%%%%%
\section{Abstract}

This report contains the implementation of an emoticon predictor, based on text. This is a project within Natural Language Processing (NLP), meaning many resources within the NLP community were consulted. This report discusses two different ways of prediction emotions. In the first case, emoticons are predicted based on a single short message. This lead to the use of an average multiclass perceptron. In the second case, chat data is used to predict emoticons, based not only on the current message, but also on the previous ones as well. This lead to the use of a Hidden Markov Model and the Viterbi decoding algorithm.


%%%%%%%%%%%%%%%%
%%%%% Introcuction %%%%%
%%%%%%%%%%%%%%%%
\section{Introduction}



%%%%%%%%%%%%%%%%
%%%%%  Problem  %%%%%%
%%%%%%%%%%%%%%%%
\section{Problem}

As stated in the introduction, this report discusses two related, but different problems - emoticon prediction based on a single short message and based on a sequence of messages. In the first problem, some amount of unrelated short messages should be observed, together with the contained emoticons, a classifier should be trained and at the end, given a new message the corresponding emotion should be predicted. This means that the messages might (and should) be written by different authors in order to achieve good generalization. In the second problem, a whole conversation should be observed and the classification should be done not only by considering the features of a single message, but instead, by also considering the previous messages of the same user. This problem is a more complex and interesting one, since it also takes into account the transitions from one sentiment state to another. \\

\noindent As a natural consequence of the above statements of the two problems, two different data sets should be used for them. For the first one it might be any corpora of text messages, containing a decent amount of emoticons. Such corpora might contain for example any unrelated text messages from social media. For the current research, a Twitter data was chosen. For the second problem, a corpora containing longer sequences of text messages should be used - these might be different conversations, such as an e-mail exchange or just chat data, as long as they contain sufficient amount of emoticons. For the current research an Ubuntu chat data was chosen. \\

\noindent After the data is collected, it is first preprocessed. Afterwards appropriate features are extracted and a model is trained. The current report does not discuss the choice of features in detail. Some basic feature were chosen for this project without intentional aim on improving them. Since this task on its own deserves a lot of time and effort, it is left as future work and suggestions about it are mentioned in the following sections. \\

\ldots \ldots \ldots 

%%% Continue the rest of the discussion of the problem by only mentioning something brief about the models and possible alternatives. More details are in the corresponding sections!!



\pagebreak


%%%%%%%%%%%%%%%%
%%%%%% Twitter  %%%%%%
%%%%%%%%%%%%%%%%
\section{Twitter emoticon prediction}

In this part of the project Twitter data was used in order to train a classifier for predicting emoticons. Three basic classes were used in order to classify the data:

\begin{itemize}
% TODO: Fix the emoticon examples to match the emoticons actually used
\item \textbf{positive} - data containing `positive' emoticons (expressing positive emotions) such as `:)', `:]', `=)', or `:D'
\item \textbf{neutral} - data containing no emoticons or neutral emoticons like `:$\vert$' or `o\_o'
\item \textbf{negative} - data containing `negative' emoticons (expressing negative emotions) such as `:(', `:[', `=(` or `;(' 
\end{itemize}

wijzers


%%%%%%%%%%%%%%%%
%%%%%% Data %%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Data}

In order to collect data from Twitter, a crawler was used. The Twitter crawler searched for training and test data, based on emoticons. These messages (`tweets') contain much information - the name of the author, text, hyperlinks, hashtags (sequences of the form `\#word\_sequence'). Since there are no official restrictions, messages often contain plenty of intentional or - more often - unintentional spelling and grammar errors. Messages also sometimes contain words from other languages, which makes preprocessing and analyzing the data even harder.


%%%%%%%%%%%%%%%%
%%%% Preprocessing %%%%%
%%%%%%%%%%%%%%%%
\subsection{Preprocessing}

Before any of the data can be used, the messages need to be preprocessed. This process includes the following steps:

\begin{itemize}
% TODO: PRECISELY EXPLAIN THE PREPROCESSING HERE :) 
\item \textbf{Splitting the message} into different components - text, hashtags and emoticons.
\item \textbf{Spelling check and correction} 
\item \textbf{Grammar check and correction} 
\end{itemize}


%%%%%%%%%%%%%%%%
%%%%%%  Model  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Model}


A multi-class perceptron was chosen as most appropriate for the purpose of this project. 


%%%%%%%%%%%%%%%%
%%%%%  Features  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Features\label{sec:feat}}

The following features were extracted from the data and used for classification:

\begin{itemize}
\item \textbf{words} - total number of words in the text
\item \textbf{positive words} - total number of positive words in the text
\item \textbf{negative words} - total number of negative words in the text
\item \textbf{positive words hashtags} - total number of positive words in the hashtags
\item \textbf{negative words hashtags} - total number of negative words in the hashtags
\item \textbf{uppercase words} - number of words containing only uppercase letters
\item \textbf{special punctuation} - number of special punctuation marks such as `!' and `?'
\item \textbf{adjectives} - number of adjectives in the text 
\end{itemize}

\noindent Extracting features such as \textbf{positive words} and \textbf{negative words} requires the usage of a predefined lists of `positive' and `negative' words. Both these lists contain ...... % TODO: DESCRIBE THE LISTS AND STATE WERE THEY WERE TAKEN FROM 
\\
\\
Features such as the number of \textbf{adjectives} were extracted by first performing POS tagging on each sentence and then counting the adjectives in the tagged sentence. Due to spelling and grammar errors in the messages, POS tagging does not always perform  perfect. As a consequence, slight inaccuracies are possible for this feature, though not significant enough to influence the performance.



%%%%%%%%%%%%%%%%
%%%%%% Results %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Experiments and Results}

\pagebreak



%%%%%%%%%%%%%%%%
%%%%%% Ubuntu %%%%%%
%%%%%%%%%%%%%%%%
\section{Ubuntu emoticon prediction}



%%%%%%%%%%%%%%%%
%%%%%% Data %%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Data}



%%%%%%%%%%%%%%%%
%%%% Preprocessing %%%%%
%%%%%%%%%%%%%%%%
\subsection{Preprocessing}

Before any of the data can be used, the messages need to be preprocessed. 



%%%%%%%%%%%%%%%%
%%%%%%  Model  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Model}


A Hidden Markov Model was chosen as post appropriate for the purpose of this project. This section summarizes the steps which were taken in order to train the model and predict classes for unseen examples afterwards. \\

\noindent As explained earlier, after the data has been pre-processed, a feature vector is extracted for each example message. Thus, the training data for the model consists of pair $(X, y)$, where $X$ is a feature vector and $y$ is the corresponding class label. The next step is to compute the transition and emission probabilities, i.e. the probability of transitioning from one class state to another and the probabilities of the different feature vectors given a class state. \\

\noindent  Consider a simple example: the probability of observing a `sad' massage right after a `neutral' message has been observed is a transition probability. Furthermore, the probability of the feature vector of the message `I need to get some sleep now' given the neutral class is an emission probability. This means that the following general formulas might be used in order to calculate these probabilities:

\begin{align*}
\cal{P} (\textit{transitioning from class y' to class y''}) 
&= \frac {\# \textit{ of times class y'' is observed after class y'}} {\textit {total \# of training examples}} \\
\cal{P} (\textit{feature vector $X$ given class y}) 
&= \frac {\# \textit{ of times the pair $(X, y)$ occurs in the training data }} {\textit {total \# of training examples from class y}} \\
\end{align*}

\noindent Since the total number of feature vectors $X$ which can be observed is too big, not all possible vectors would be observed in the training data. This would make predicting unseen messages impossible, since no emission probabilities would be present for these vectors. In order to avoid this issue, the following solution is implemented: the training data is first clustered, emission probabilities are computed for each cluster, then given an unseen message, its feature vector is first assigned to a cluster and finally, a label is predicted for this message. \\

\noindent 


%%%%%%%%%%%%%%%%
%%%%%  Features  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Features}

The features used in this part of the project correspond to the ones already discussed in section ~\ref{sec:feat}. Note that some features, such as numbers of positive or negative words in hashtags are not applicable in the current setting, since the data does not contain hashtags. Thus, such features have not been considered. 


%%%%%%%%%%%%%%%%
%%%%%% Results %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Experiments and Results}

\pagebreak



%%%%%%%%%%%%%%%%
%%%%  Conculsion  %%%%%%
%%%%%%%%%%%%%%%%
\section{Conclusion}




%%%% He has to be kidding about this one, but... :/ 
\section{Team responsibilities}


\section*{Files attached}
\begin{itemize}
\item Code
\end{itemize}

\section*{Sources}
\addcontentsline{toc}{section}{Sources}
\begin{itemize}
	\item Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc.
	\item Author: Ryan Kelly, Home Page: https://pythonhosted.org/pyenchant/
\end{itemize}


I'm gonna cite something here \cite{greenwade93} to see how it works ;)

\nocite{*}
\printbibliography
% The bibliography still contains useless things. To be changed later

\begin{itemize}

\item Some paper
\end{itemize}

\end{document}
