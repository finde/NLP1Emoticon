\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor,colortbl}
\usepackage[]{algorithm2e}

%\usepackage{soul}

\newcommand{\tab}{\hspace{10mm}}
\newcommand{\dtab}{\hspace{20mm}}
\newcommand{\ttab}{\hspace{30mm}}
\newcommand{\qtab}{\hspace{40mm}}

\begin{document}

\title{Natural Language Processing \\ Emoticon Prediction}

\author{By: Gornishka, Loor, Qodari, Xumara}
\maketitle


\tableofcontents

\pagebreak


%%%%%%%%%%%%%%%%
%%%%% Introcuction %%%%%
%%%%%%%%%%%%%%%%
\section{Introduction}

This report contains the implementation of an emoticon predictor, based on text. This is a project within Natural Language Processing (NLP), meaning many resources within the NLP community were consulted. This report discusses two different ways of prediction emotions. In the first case, emoticons are predicted based on a short message. This lead to the use of an average multiclass perceptron. In the second case, chat data is used to predict emoticons, based on the message as well as the previous post. This lead to the use of the Viterbi algorithm.


%%%%%%%%%%%%%%%%
%%%%% Assignment %%%%%
%%%%%%%%%%%%%%%%
\section{Assignment}

As stated in the introduction, this report discusses emoticon prediction based on a short message and based on the previous message. In order to find small messages, Twitter was used. For the second part of the assignment, Ubuntu chat data was used.

\pagebreak


%%%%%%%%%%%%%%%%
%%%%%% Twitter  %%%%%%
%%%%%%%%%%%%%%%%
\section{Twitter emoticon prediction}

In this part of the project Twitter data was used in order to train a classifier for predicting emoticons. Three basic classes were used in order to classify the data:

\begin{itemize}
% TODO: Fix the emoticon examples to match the emoticons actually used
\item \textbf{positive} - data containing `positive' emoticons (expressing positive emotions) such as `:)', `:]', `=)', or `:D'
\item \textbf{neutral} - data containing no emoticons or neutral emoticons like `:$\vert$' or `o\_o'
\item \textbf{negative} - data containing `negative' emoticons (expressing negative emotions) such as `:(', `:[', `=(` or `;(' 
\end{itemize}

wijzers


%%%%%%%%%%%%%%%%
%%%%%% Data %%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Data}

In order to collect data from Twitter, a crawler was used. The Twitter crawler searched for training and test data, based on emoticons. These messages (`tweets') contain much information - the name of the author, text, hyperlinks, hashtags (sequences of the form `\#word\_sequence'). Since there are no official restrictions, messages often contain plenty of intentional or - more often - unintentional spelling and grammar errors. Messages also sometimes contain words from other languages, which makes preprocessing and analyzing the data even harder.


%%%%%%%%%%%%%%%%
%%%% Preprocessing %%%%%
%%%%%%%%%%%%%%%%
\subsection{Preprocessing}

Before any of the data can be used, the messages need to be preprocessed. This process includes the following steps:

\begin{itemize}
% TODO: PRECISELY EXPLAIN THE PREPROCESSING HERE :) 
\item \textbf{Splitting the message} into different components - text, hashtags and emoticons.
\item \textbf{Spelling check and correction} 
\item \textbf{Grammar check and correction} 
\end{itemize}


%%%%%%%%%%%%%%%%
%%%%%%  Model  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Model}


A multi-class perceptron was chosen as most appropriate for the purpose of this project. 


%%%%%%%%%%%%%%%%
%%%%%  Features  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Features}

The following features were extracted from the data and used for classification:

\begin{itemize}
\item \textbf{words} - total number of words in the text
\item \textbf{positive words} - total number of positive words in the text
\item \textbf{negative words} - total number of negative words in the text
\item \textbf{positive words hashtags} - total number of positive words in the hashtags
\item \textbf{negative words hashtags} - total number of negative words in the hashtags
\item \textbf{uppercase words} - number of words containing only uppercase letters
\item \textbf{special punctuation} - number of special punctuation marks such as `!' and `?'
\item \textbf{adjectives} - number of adjectives in the text 
\end{itemize}

\noindent Extracting features such as \textbf{positive words} and \textbf{negative words} requires the usage of a predefined lists of `positive' and `negative' words. Both these lists contain ...... % TODO: DESCRIBE THE LISTS AND STATE WERE THEY WERE TAKEN FROM 
\\
\\
Features such as the number of \textbf{adjectives} were extracted by first performing POS tagging on each sentence and then counting the adjectives in the tagged sentence. Due to spelling and grammar errors in the messages, POS tagging does not always perform  perfect. As a consequence, slight inaccuracies are possible for this feature, though not significant enough to influence the performance.



%%%%%%%%%%%%%%%%
%%%%%% Results %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Results}

\pagebreak



%%%%%%%%%%%%%%%%
%%%%%% Ubuntu %%%%%%
%%%%%%%%%%%%%%%%
\section{Ubuntu emoticon prediction}



%%%%%%%%%%%%%%%%
%%%%%% Data %%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Data}



%%%%%%%%%%%%%%%%
%%%% Preprocessing %%%%%
%%%%%%%%%%%%%%%%
\subsection{Preprocessing}

Before any of the data can be used, the messages need to be preprocessed. 



%%%%%%%%%%%%%%%%
%%%%%%  Model  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Model}


A Hidden Markov Model was chosen as post appropriate for the purpose of this project. This section summarizes the steps which were taken in order to train the model and predict classes for unseen examples afterwards. \\

\noindent As explained earlier, after the data has been pre-processed, a feature vector is extracted for each example message. Thus, the training data for the model consists of pair $(X, y)$, where $X$ is a feature vector and $y$ is the corresponding class label. The next step is to compute the transition and emission probabilities, i.e. the probability of transitioning from one class state to another and the probabilities of the different feature vectors given a class state. \\

\noindent  Consider a simple example: the probability of observing a `sad' massage right after a `neutral' message has been observed is a transition probability. Furthermore, the probability of the feature vector of the message `I need to get some sleep now' given the neutral class is an emission probability. This means that the following general formulas might be used in order to calculate these probabilities:

\begin{align*}
\cal{P} (\textit{transitioning from class y' to class y''}) 
&= \frac {\# \textit{ of times class y'' is observed after class y'}} {\textit {total \# of training examples}} \\
\cal{P} (\textit{feature vector $X$ given class y}) 
&= \frac {\# \textit{ of times the pair $(X, y)$ occurs in the training data }} {\textit {total \# of training examples from class y}} \\
\end{align*}

\noindent Since the total number of feature vectors $X$ which can be observed is too big, not all possible vectors would be observed in the training data. This would make predicting unseen messages impossible, since no emission probabilities would be present for these vectors. In order to avoid this issue, the following solution is implemented: the training data is first clustered, emission probabilities are computed for each cluster, then given an unseen message, its feature vector is first assigned to a cluster and finally, a label is predicted for this message. \\

\noindent 


%%%%%%%%%%%%%%%%
%%%%%  Features  %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Features}



%%%%%%%%%%%%%%%%
%%%%%% Results %%%%%%
%%%%%%%%%%%%%%%%
\subsection{Results}

\pagebreak



%%%%%%%%%%%%%%%%
%%%%  Conculsion  %%%%%%
%%%%%%%%%%%%%%%%
\section{Conclusion}





\section*{Files attached}
\begin{itemize}
\item Code
\end{itemize}

\section*{Sources}
\addcontentsline{toc}{section}{Sources}

\begin{itemize}

\item Some paper
\end{itemize}

\end{document}
