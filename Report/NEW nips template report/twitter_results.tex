
% Requirements:
\begin{comment}
Experiments / Empirical evaluation (roughly 2-3 pages)
• Any details about experiments (dataset sizes, parameter selection, etc)
• Results
• Analysis (discussion of results / visualization / findings / etc)
\end{comment}

As the AMP needs to be used to relabel the Ubuntu chat data, different experiments were run to test the performance. Sample results are shown in table \ref{table:AMPaccuracy} and are be discussed in detail in this section. Each test is run based on a standard set of 500 samples per class to ensure balance. 80\% of the data is used for training and 20\% is used as a test set to measure the performance of the model. In order to ensure a proper resemblance of test results, the results are averaged over 10 tests. Also, all features as declared earlier are used.

\subsubsection{Prediction with varying number of classes}
To test how well the perceptron predicts data on a varying number of classes, the algorithm was run while predicting multiple classes. Initially, it is interesting to see how well the data is classified using two classes; positive and negative. There is also a third class representing neutral data. There are also classes which are defined as extremely positive and extremely negative. In total, these are 15 classes. The performance of the model is also tested in this number of classes. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c | c | c |}
\hline
 {\textbf{Classes}} 	 
 & {\textbf{Data per class}} 					& {\textbf{Iterations}} 
 & {\textbf{Train Accuracy}} 					& {\textbf{Test Accuracy}} 
 \\
\hline
2	&	500	&	50	&	52.60	& 	51.95	\\
3	&	500	&	50	&	38.93	&	37.73	\\
15	&	500	&	50	&	10.62	&	9.01		\\
\hline
\end{tabular}
\caption{AMP accuracy for different number of classes }
\label{table:AMPaccuracy}
\end{center}
\end{table}

\begin{comment}
\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
 {\textbf{Classes}} 			& {\textbf{Features}} 
 & {\textbf{Data per class}} 					& {\textbf{Iterations}} 
 & {\textbf{Train Accuracy}} 					& {\textbf{Test Accuracy}} 
 \\
\hline
2 			& all 				& 500 		& 50			& ?			& ? 			\\
2 			& subset 		& 500 		& 50			& ?			& ? 			\\
3 			& all 				& 500 		& 50			& ?			& ? 			\\
3 			& subset 		& 500 		& 50			& ?			& ? 			\\
15 		& all		 		& 500 		& 50			& ?			& ? 			\\
15 		& subset 		& 500 		& 50			& ?			& ? 			\\
\hline
\end{tabular}
\caption{AMP accuracy}
\label{table:AMPaccuracy}
\end{center}
\end{table}
\end{comment}

\subsubsection*{Classes} 
Unsurprisingly, the results show that the more classes are used the worse the performance becomes. \\

\noindent In order to test the performance on two classes, the messages containing different`happy' and `sad' emoticons were split into two general groups - `negative' and `positive' data. Since the data in these classes is significantly different, especially with respect to features such as count of positive or negative words, distinguishing differences seems to be much easier. \\

\noindent For testing the performance on three classes, data containing no emoticons or neutral emoticons were added to the training set as `neutral' data. In this case the performance dropped dramatically. The lack of an emoticon in a message does not necessarily mean that this message is not a positive or a negative one, which is reflected in the results. For example, some people do not use emoticons to express their emotions, but hashtags instead. Thus, neutral data is often classified mistakenly as positive or negative or vice versa, which significantly decreases the overall performance of the perceptron. \\

\noindent For testing purposes, the data was eventually split into 15 different classes, each of them containing messages with a different emoticon - i.e. messages containing emoticons such as `:)' and `:D' were not both considered `positive' anymore, but treated as different classes instead. This has lead to very small datasets for certain classes. This leads to bias of the larger classes, mislabelling most data.


\subsubsection*{Features}

The effect of different features was also explored. Each one of the features was being excluded and prediction performance was tested using the rest of the features. In order to see clear results, two classes were used to classify the data. These results are displayed in table \ref{table:AMPfeatures}.

\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c | c | c |}
\hline
 {\textbf{Features Excluded}} 	 
 & {\textbf{Data per class}} 					& {\textbf{Iterations}} 
 & {\textbf{Train Accuracy}} 					& {\textbf{Test Accuracy}} 
 \\
\hline
None		&				500	&	50	&	52.60	& 	51.95	\\
Positive/negative words	&		500	&	50	&	53.88	&	52.90	\\
Positive/negative hashtags	&	500	&	50	&	54.40	&	53.70	\\
Amount of words	&		500	&	50	&	51.78	&	50.90	\\
Uppercase words	&		500	&	50	&	51.51	&	49.65	\\
Special punctuation	&	500	&	50	&	51.91	&	51.25	\\
Adjectives	&			500	&	50	&	52.39	&	50.25	\\
\hline
\end{tabular}
\caption{AMP accuracy}
\label{table:AMPfeatures}
\end{center}
\end{table}


Tests show that some of the features are not meaningful enough and including them only decreases the performance. For example, the use of hashtags should improve the performance, but it appears that when emoticons are used, hashtags aren't. Therefore, the features that represent the hashtags are mainly empty. This does not help distinguish positive data from negative data and should be excluded from the feature set. As this model is used to label data which does not contain hashtags, this should not make a difference. Also, the sequence-based data hardly contains words represented solely in uppercase. Therefore, this feature can also be excluded from the feature set. This leads to the following results

\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c | c | c |}
\hline
 {\textbf{Features Excluded}} 	 
 & {\textbf{Data per class}} 					& {\textbf{Iterations}} 
 & {\textbf{Train Accuracy}} 					& {\textbf{Test Accuracy}} 
 \\
\hline
\parbox[t]{3cm}{Positive/negative hashtags and Uppercase words}	&	500	&	50	&	56.96	& 	56.25	\\
\hline
\end{tabular}
\caption{AMP optimal accuracy}
\label{table:AMPoptimal}
\end{center}
\end{table}

Excluding the positive and negative hashtags, as well as the uppercase words, yield the best results and will be used for relabelling the sequence-based data.

\begin{comment}
Tests show that some of the features are not meaningful enough and including them in the set of used features only decreases the performance. Furthermore, the Average Multiclass Perceptron was also used further in this project in a scenario where the data does not contain any hashtags. Thus, in table \ref{table:AMPfeatures}, results are shown for two different feature sets. The first one, denoted as `all', contains all features discussed in section \ref{sec:features}.
The second one, denoted in table \ref{table:AMPaccuracy} as `subset', is a subset of all features, which does not contain hashtag-related features \red{and maybe the uppercase words since ubuntu data does not contain much of them? shall we try also without it}.
\end{comment}

\subsection{Dataset size}
This section discusses the effect of different dataset sizes on the AMP implementation. The results are presented in table \ref{table:AMPdataset}. As hashtags are hardly used, this feature was turned off to research the sizes of the dataset. This will create less similarity between the classes.

\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
 {\textbf{Classes}} 	 
 & {\textbf{Samples per class}} 					& {\textbf{Clusters}} 
 & {\textbf{Train Accuracy (\%)}} 					& {\textbf{Test Accuracy (\%)}} 
 \\
\hline
2 	 		& 50			& 50			& 	59.75	& 48.00		\\
2 	 		& 100 		& 50			& 	55.75	& 52.25		\\
2 	 		& 200 		& 50			& 	55.19	& 53.63		\\
2 	 		& 300 		& 50			& 	54.65	& 53.75		\\
2 	 		& 400 		& 50			& 	54.55	& 52.56		\\
2 	 		& 500 		& 50			& 	55.78	& 54.80		\\
2 	 		& 1000 		& 50			& 	53.47	& 53.80		\\
2 	 		& 2500 		& 50			& 	54.68	& 54.82		\\
\hline
\end{tabular}
\caption{AMP size of dataset}
\label{table:AMPdataset}
\end{center}
\end{table}

Table \ref{table:AMPdataset} indicates that the more data is used, the more the model generalizes. However, the performance of the model remains poor. This happens due to poorly chosen features. Also, the model seems unstable as there is no clear increase and decrease in performance. However, as the difference in performance is smallest when using a dataset of 2500 samples per class, this indicates that generalization is being achieved.