
% Requirements:
\begin{comment}
Problem: (roughly 1-2 pages)
â€¢ Explain the problem; what kind of assumptions / observations you have about the problem

\red{Find out what should be actually in this section} \\
\end{comment}

The intent of this research is to determine the transition of emoticons in a conversation. Such transitions are found in sequential messages. As stated before, chat data from Ubuntu is used to analyse transitions. This leads to the following problem: 
\begin{center}
\textit{How can emoticons be predicted in a sequence of messages, using the emoticon of the previous message?}
\end{center}

This leads to several, obvious, steps that must be taken. These are:
\begin{itemize}
\item Preprocess corpora
\item Extract features
\item Implement a model
\end{itemize}

However, the selected corpora is not optimal. The Ubuntu chat data does not contain enough emoticons. Therefore, in order to train a model, each message must be classified. This leads to implementing another model which labels the Ubuntu chat corpora. This model must determine the emoticon of a single, independent message. This means that a different model must be trained on a different corpora. Using Twitter data, the use of emoticons can be used to train a such a model. This model will be used to label the Ubuntu data. Then, the emoticons of the sequential messages must be predicted.

As stated before, both corpora must be preprocessed. As the messages in both corpora are written by users in their spare time, the presence of grammatical- and spelling errors and slang is quite real. There are also other sub-optimal aspects of the data that must be handled. After preprocessing, the corpora are ready for feature extraction. These subjects will be discussed where necessary.

When training a model to predict emoticons on independent messages, there are several models that can be used. A support vector machine (SVM) and perceptron can be used. Therein, different perceptrons can be used. For this research, a perceptron was used. Two perceptrons were implemented: the average multiclass perceptron (AMP) and the multilayer perceptron (MLP). The AMP can be used to classify the data. However, the MLP can yield better results due to the presence of hidden layers. This research will show which yields best results.

The model used to predict emoticons on sequences is the Hidden Markov Model (HMM). There are two versions of the HMM: the first-order HMM and the second-order HMM. The first-order HMM uses the emoticon of the previous message to predict the emoticon of the message that is being analysed. The second-order HMM uses the two previous messages to predict the emoticon of the message that is being analysed. To conduct this research, the first-order HMM is used. This can be expanded in the future to take into account two messages.

It is expected that the two models will work separately. Even if the first model, used to relabel the sequence-based data, performs poorly, the performance of the HMM will not be affected. As the HMM is based on mislabelled data, it will learn these labels and continue building upon this. This will lead to sub-optimal results, overall, but the HMM will still perform well. Also, the sequence-based data in itself is slightly biased. As this data was collected from a support forum, it is expected that most data will be negative. This can lead to biased prediction. This must be prevented, by creating a balanced dataset of positive, negative and neutral data. 

The dataset must be balanced for both models. However, creating a balanced dataset is easier the single-message prediction as there are no transitions there. This means that selecting data to train the HMM on must be performed very well to create a purely balanced dataset.

To summarize: in order to predict emoticons in sequence of messages, the positivity, neutrality or negativity of each message must be determined. By preprocessing each message and extracting features, the "gist" of each message is extracted. This "gist" is used to train a first-order Hidden Markov Model, which can then be used to predict the emoticon of messages. However, in order to label the training data correctly, first all messages must be labelled. To do this, a perceptron is used. When training all of the models, it is imperative to select balanced data to train each model in order to achieve optimal prediction.
