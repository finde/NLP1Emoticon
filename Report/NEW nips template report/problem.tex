
% Requirements:
\begin{comment}
Problem: (roughly 1-2 pages)
â€¢ Explain the problem; what kind of assumptions / observations you have about the problem

\red{Find out what should be actually in this section} \\
\end{comment}

The intent of this research is to determine the transition of emoticons in a conversation. Such transitions are found in sequential messages. As stated before, chat data from Ubuntu is used to analyse transitions. This leads to the following problem: 
\begin{center}
\textit{How can emoticons be predicted in a sequence of messages, using the emoticon of the previous message?}
\end{center}

This leads to several obvious steps that must be taken. These are:
\begin{itemize}
\item Preprocess corpora
\item Extract features
\item Train a model
\item Predict emoticons for unseen data
\end{itemize}

However, the Ubuntu chat data does not contain enough emoticons. Therefore, before training a model, each message which does not contain an emoticon must first be relabeled. This requires the implementation of another model which, given a single, independent message, predicts a corresponding emoticon. This means that a different model must be trained on a different corpus. As mentioned before, a Twitter data can be used for training this model. Once emoticons are predicted for the Ubuntu chat messages, the newly obtained corpus might be eventually used for emoticon prediction in a sequence of messages.


As stated before, both corpora must be preprocessed. As the messages in both corpora are written by users in their spare time, there is a presence of slang, grammatical- and spelling errors. There are also other sub-optimal aspects of the data that must be handled. After preprocessing, the corpora are ready for feature extraction. These subjects will be discussed later in this paper.


\red{I am not even going to try to fix the next fucking paragraph, please somebody do something about it because I would kill myself about 7 times before fixing it.}
When training a model to predict emoticons on independent messages, there are several models that can be used. A support vector machine (SVM) and perceptron can be used. Therein, different perceptrons can be used. For this research, a perceptron was used. Two perceptrons were implemented: the average multiclass perceptron (AMP) and the multilayer perceptron (MLP). The AMP can be used to classify the data. However, the MLP can yield better results due to the presence of hidden layers. This research will show which yields best results.

The model used to predict emoticons on sequences is the Hidden Markov Model (HMM). There are two versions of the HMM: the first-order HMM and the second-order HMM. The first-order HMM uses the emoticon of the previous message to predict the emoticon of the message that is being analysed, whereas the second-order HMM uses the two previous messages. To conduct this research, the first-order HMM is used. This can be expanded in the future to take into account two messages.

It is expected that the two models will work separately. Even if the first model, used to relabel the sequence-based data, performs poorly, the performance of the HMM will not be affected. As the HMM is based on mislabelled data, it will learn these labels and continue building upon this. This will lead to sub-optimal results, overall, but the HMM will still perform well. Also, the sequence-based data in itself is slightly biased. As this data was collected from a support forum, it is expected that most data will be negative. This can lead to biased prediction. This must be prevented, by creating a balanced dataset of positive, negative and neutral data. 

\red{Scottish guy's response for the following paragraph: `I'm sorry, that's not English'}
The datasets must be balanced for both models. However, creating a balanced dataset is easier the single-message prediction as there are no transitions there. This means that selecting data to train the HMM on must be performed very well to create a purely balanced dataset.


% This is all explained in this section, no need to summarize it like 4 lines later
\begin{comment}
To summarize: in order to predict emoticons in sequence of messages, the positivity, neutrality or negativity of each message must be determined. By preprocessing each message and extracting features, the "gist" of each message is extracted. This "gist" is used to train a first-order Hidden Markov Model, which can then be used to predict the emoticon of messages. However, in order to label the training data correctly, first all messages must be labelled. To do this, a perceptron is used. When training all of the models, it is imperative to select balanced data to train each model in order to achieve optimal prediction.
\end{comment}
